\documentclass[12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\usepackage{enumitem}
\geometry{margin=1in}

\title{\textbf{CSE400 â€“ Fundamentals of Probability in Computing}\\
Lecture 7: Expectation, CDFs, PDFs and Problem Solving Skills}
\author{Instructor: Dhaval Patel, PhD}
\date{January 27, 2025}

\begin{document}
\maketitle

\hrule
\vspace{1em}

\section{Outline of the Lecture}

\begin{itemize}
    \item The Cumulative Distribution Function (CDF)
    \begin{itemize}
        \item Definition
        \item Properties
        \item Examples
    \end{itemize}

    \item The Probability Density Function (PDF)
    \begin{itemize}
        \item Definition
        \item PDF--CDF relationship
        \item Properties
        \item Examples
    \end{itemize}

    \item Expectation of Random Variables
    \begin{itemize}
        \item Definition
        \item Expectation of a function of a random variable
        \item Linear operations with expectation
    \end{itemize}

    \item $n^{\text{th}}$ moments and central moments of random variables
    \begin{itemize}
        \item Variance
        \item Skewness
        \item Kurtosis
    \end{itemize}
\end{itemize}

\section{CDF and PDF: Intuition (Water Tank Analogy)}

\begin{itemize}
    \item The cumulative distribution function (CDF) is explained using a water tank analogy.
    \item The height $(h)$ of water in the tank represents the value of a random variable.
    \item The volume of water up to height $(h)$, denoted by $V(h)$, corresponds to the cumulative probability up to that value.
\end{itemize}

\subsection*{Mathematical Representation}

\[
V(h) = \int_{0}^{h} \pi R^2 \, dh = (\pi R^2)h
\]

\begin{itemize}
    \item Here, $\pi R^2$ is analogous to the probability density function (PDF) of a uniform distribution.
\end{itemize}

The maximum volume of the tank is:
\[
V(H) = \pi R^2 H
\]

This maximum volume is analogous to total probability equal to 1.

\section{Cumulative Distribution Function (CDF)}

\subsection{Definition}

The cumulative distribution function (CDF) of a random variable $X$ is defined as:
\[
F_X(x) = \Pr(X < x), \quad -\infty < x < \infty
\]

Most of the information about the random experiment described by the random variable $X$ is determined by the behavior of $F_X(x)$.

\subsection{Properties of the CDF}

The CDF satisfies the following properties:

\begin{itemize}
    \item $0 \leq F_X(x) \leq 1$
    \item $F_X(-\infty) = 0$
    \item $F_X(\infty) = 1$
    \item If $x_1 < x_2$, then:
    \[
    F_X(x_1) \leq F_X(x_2)
    \]
    \item For $x_1 < x_2$:
    \[
    \Pr(x_1 < X < x_2) = F_X(x_2) - F_X(x_1)
    \]
\end{itemize}

\subsection{CDF Example \#1: Validity of CDFs}

\textbf{Question:} Find the valid CDF.

Given candidate functions:
\begin{enumerate}
    \item $F_X(x) = 3 + 2\tan^{-1}(x)$
    \item $F_X(x) = [1 - e^{-x}]u(x)$
    \item $F_X(x) = e^{x^2}$
    \item Other given expressions as shown in slides
\end{enumerate}

Validity is checked using CDF properties:
\begin{itemize}
    \item Range between 0 and 1
    \item Proper limits at $-\infty$ and $+\infty$
    \item Monotonic non-decreasing behavior
\end{itemize}

\subsection{CDF Example \#2}

Given:
\[
F_X(x) = (1 - e^{-x})u(x)
\]

Find the following probabilities:
\begin{itemize}
    \item $\Pr(X > 5)$
    \item $\Pr(X < 5)$
    \item $\Pr(3 < X < 7)$
    \item $\Pr(X > 5 \mid X < 7)$
\end{itemize}

Each probability is computed directly using:
\[
\Pr(a < X < b) = F_X(b) - F_X(a)
\]

\section{Probability Density Function (PDF)}

\subsection{Definition and PDF--CDF Relationship}

The PDF of a continuous random variable $X$ at point $x$ is defined as:
\[
f_X(x) = \lim_{\epsilon \to 0} \frac{\Pr(x < X < x+\epsilon)}{\epsilon}
\]

For a continuous range:
\[
\Pr(x < X < x+\epsilon) = F_X(x+\epsilon) - F_X(x)
\]

Substituting and taking the limit:
\[
f_X(x) = \frac{dF_X(x)}{dx}
\]

Hence:
\begin{itemize}
    \item The PDF is the derivative of the CDF.
    \item The CDF is the integral of the PDF:
    \[
    F_X(x) = \int_{-\infty}^{x} f_X(y)\, dy
    \]
\end{itemize}

\subsection{Properties of the PDF}

\begin{itemize}
    \item $f_X(x) \geq 0$
    \item $f_X(x) = \dfrac{dF_X(x)}{dx}$
    \item $F_X(x) = \int_{-\infty}^{x} f_X(y)\, dy$
    \item Total probability:
    \[
    \int_{-\infty}^{\infty} f_X(x)\, dx = 1
    \]
    \item Probability over an interval:
    \[
    \Pr(a < X < b) = \int_{a}^{b} f_X(x)\, dx
    \]
\end{itemize}

\subsection{PDF Example \#1: Validity of PDFs}

\textbf{Question:} Which of the following are valid PDFs?

Candidate functions include:
\begin{itemize}
    \item $f_X(x) = e^{-x}u(x)$
    \item Piecewise-defined functions over bounded intervals
    \item $f_X(x) = 2x e^{-x}u(x)$
\end{itemize}

Validity is determined by checking:
\begin{itemize}
    \item Non-negativity
    \item Integral over entire range equals 1
\end{itemize}

\subsection{PDF Example \#2}

Given CDF:
\[
F_X(x) = (1 - e^{-x})u(x)
\]

The PDF is obtained by differentiation:
\[
f_X(x) = \frac{dF_X(x)}{dx} = e^{-x}u(x)
\]

Conversely, if:
\[
f_X(x) = 2x e^{-x^2}u(x)
\]

Then the CDF is:
\[
F_X(x) = \int_{0}^{x} 2y e^{-y^2} dy = (1 - e^{-x^2})u(x)
\]

\section{Expectation of Random Variables}

\subsection{Expectation}

Expectation of a random variable $X$ is denoted by:
\[
\mathbb{E}[X]
\]

The expectation operator is linear.

\subsection{Expectation of a Function of a Random Variable}

For a function $g(X)$:
\[
\mathbb{E}[g(X)]
\]

The definition follows directly from the PDF-based formulation as presented in the slides.

\subsection{Linear Operations with Expectation}

Expectation satisfies linearity:
\[
\mathbb{E}[aX + b] = a\mathbb{E}[X] + b
\]

\section{Moments and Central Moments of Random Variables}

\begin{itemize}
    \item $n^{\text{th}}$ moments and central moments are introduced.
    \item These include:
    \begin{itemize}
        \item Variance
        \item Skewness
        \item Kurtosis
    \end{itemize}
\end{itemize}

\vspace{1em}
\hrule
\vspace{0.5em}
\centerline{\textbf{End of Lecture 7 Scribe}}

\end{document}
